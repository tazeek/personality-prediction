{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sota_list import LSTMNetwork\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoConfig\n",
    "\n",
    "import torch\n",
    "import codecs\n",
    "import json\n",
    "from pprint import pprint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model weights\n",
    "model_weights_path = \"finetuned_saved_models/bilstm-bert-finetuned-segmented-extracted.pth\"\n",
    "\n",
    "# PML\n",
    "plm_name = 'bert-finetuned-segmented'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_llm_parts(model_name):\n",
    "\n",
    "    # Load the config, model, and tokenizer\n",
    "    config = AutoConfig.from_pretrained(model_name, output_hidden_states =True)\n",
    "\n",
    "    return [\n",
    "        AutoModelForSequenceClassification.from_pretrained(model_name, config=config),\n",
    "        AutoTokenizer.from_pretrained(model_name)\n",
    "    ]\n",
    "\n",
    "\n",
    "\n",
    "# Initialize the model and load the weights\n",
    "model = LSTMNetwork(768,128,5,True)\n",
    "model.load_state_dict(torch.load(model_weights_path))\n",
    "model.eval()\n",
    "\n",
    "# Load the LLM fine-tuned model\n",
    "llm_model, tokenizer = load_llm_parts(plm_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'emotion': 'neutral',\n",
      "  'sentiment': '0',\n",
      "  'speaker': 'Chandler',\n",
      "  'utterance': \"also I was the point person on my company's transition from \"\n",
      "               'the KL-5 to GR-6 system.'},\n",
      " {'emotion': 'neutral',\n",
      "  'sentiment': '0',\n",
      "  'speaker': 'The Interviewer',\n",
      "  'utterance': \"You must've had your hands full.\"},\n",
      " {'emotion': 'neutral',\n",
      "  'sentiment': '0',\n",
      "  'speaker': 'Chandler',\n",
      "  'utterance': 'That I did. That I did.'},\n",
      " {'emotion': 'neutral',\n",
      "  'sentiment': '0',\n",
      "  'speaker': 'The Interviewer',\n",
      "  'utterance': \"So let's talk a little bit about your duties.\"},\n",
      " {'emotion': 'surprise',\n",
      "  'sentiment': '1',\n",
      "  'speaker': 'Chandler',\n",
      "  'utterance': 'My duties?  All right.'},\n",
      " {'emotion': 'neutral',\n",
      "  'sentiment': '0',\n",
      "  'speaker': 'The Interviewer',\n",
      "  'utterance': \"Now you'll be heading a whole division, so you'll have a lot \"\n",
      "               'of duties.'},\n",
      " {'emotion': 'neutral',\n",
      "  'sentiment': '0',\n",
      "  'speaker': 'Chandler',\n",
      "  'utterance': 'I see.'},\n",
      " {'emotion': 'neutral',\n",
      "  'sentiment': '0',\n",
      "  'speaker': 'The Interviewer',\n",
      "  'utterance': \"But there'll be perhaps 30 people under you so you can dump a \"\n",
      "               'certain amount on them.'},\n",
      " {'emotion': 'neutral',\n",
      "  'sentiment': '0',\n",
      "  'speaker': 'Chandler',\n",
      "  'utterance': 'Good to know.'},\n",
      " {'emotion': 'neutral',\n",
      "  'sentiment': '0',\n",
      "  'speaker': 'The Interviewer',\n",
      "  'utterance': 'We can go into detail'},\n",
      " {'emotion': 'fear',\n",
      "  'sentiment': '2',\n",
      "  'speaker': 'Chandler',\n",
      "  'utterance': \"No don't I beg of you!\"},\n",
      " {'emotion': 'neutral',\n",
      "  'sentiment': '0',\n",
      "  'speaker': 'The Interviewer',\n",
      "  'utterance': \"All right then, we'll have a definite answer for you on \"\n",
      "               \"Monday, but I think I can say with some confidence, you'll fit \"\n",
      "               'in well here.'},\n",
      " {'emotion': 'surprise',\n",
      "  'sentiment': '1',\n",
      "  'speaker': 'Chandler',\n",
      "  'utterance': 'Really?!'},\n",
      " {'emotion': 'neutral',\n",
      "  'sentiment': '0',\n",
      "  'speaker': 'The Interviewer',\n",
      "  'utterance': 'Absolutely.  You can relax'}]\n"
     ]
    }
   ],
   "source": [
    "# Load the ERC datasets\n",
    "def load_dataset(name, type):\n",
    "    file_name = f'erc-datasets/{name}/{type}.json'\n",
    "    with codecs.open(file_name, 'r', 'utf-8') as fr:\n",
    "            return json.load(fr)\n",
    "\n",
    "    return None\n",
    "\n",
    "dataset = load_dataset('MELD','train')\n",
    "pprint(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_classification(model, llm_model, tokenizer, text):\n",
    "\n",
    "    # Tokenize\n",
    "    token_ids = tokenizer(\n",
    "        text, \n",
    "        truncation = True, \n",
    "        return_tensors='pt', \n",
    "        max_length = 512, \n",
    "        add_special_tokens=True\n",
    "    )\n",
    "\n",
    "    # Extract CLS\n",
    "    cls_output = llm_model(**token_ids)\n",
    "    cls_output = cls_output.hidden_states[-1][0,0,:]\n",
    "\n",
    "    # Get the output\n",
    "    output = model(cls_output)\n",
    "\n",
    "    # Convert to labels\n",
    "    scores = torch.sigmoid(output)\n",
    "    print(scores)\n",
    "\n",
    "    # Return\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'dim'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m conversation \u001b[38;5;129;01min\u001b[39;00m dataset:\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m utt_data \u001b[38;5;129;01min\u001b[39;00m conversation:\n\u001b[1;32m----> 4\u001b[0m         \u001b[43mperform_classification\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mllm_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mutt_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutterance\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[21], line 17\u001b[0m, in \u001b[0;36mperform_classification\u001b[1;34m(model, llm_model, tokenizer, text)\u001b[0m\n\u001b[0;32m     14\u001b[0m cls_output \u001b[38;5;241m=\u001b[39m cls_output\u001b[38;5;241m.\u001b[39mhidden_states[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m,:]\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Get the output\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcls_output\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Convert to labels\u001b[39;00m\n\u001b[0;32m     20\u001b[0m scores \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msigmoid(output)\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\ml_projects\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\ml_projects\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\Desktop\\Projects\\personality-prediction\\sota_list.py:126\u001b[0m, in \u001b[0;36mLSTMNetwork.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m    124\u001b[0m \n\u001b[0;32m    125\u001b[0m     \u001b[38;5;66;03m# Fully connected layer\u001b[39;00m\n\u001b[1;32m--> 126\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures_extraction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    128\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39msigmoid(logits)\n",
      "File \u001b[1;32mc:\\Users\\User\\Desktop\\Projects\\personality-prediction\\sota_list.py:116\u001b[0m, in \u001b[0;36mLSTMNetwork.features_extraction\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfeatures_extraction\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m    110\u001b[0m \n\u001b[0;32m    111\u001b[0m     \u001b[38;5;66;03m# Initialize hidden state with zeros\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    114\u001b[0m \n\u001b[0;32m    115\u001b[0m     \u001b[38;5;66;03m# Forward pass through LSTM\u001b[39;00m\n\u001b[1;32m--> 116\u001b[0m     out, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;66;03m# Take output from the last time step\u001b[39;00m\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;66;03m#out = out[:, -1, :]\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\ml_projects\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\ml_projects\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\ml_projects\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:875\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    873\u001b[0m         hx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[0;32m    874\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 875\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdim\u001b[49m() \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m):\n\u001b[0;32m    876\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLSTM: Expected input to be 2D or 3D, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mD instead\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    877\u001b[0m     is_batched \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'dim'"
     ]
    }
   ],
   "source": [
    "for conversation in dataset:\n",
    "    \n",
    "    for utt_data in conversation:\n",
    "        perform_classification(model, llm_model, tokenizer, utt_data['utterance'])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_projects",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
